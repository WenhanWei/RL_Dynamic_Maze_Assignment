{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Only works on Google Colab"
      ],
      "metadata": {
        "id": "FwLQVT9hqgF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "# Google Colab with Personal Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# Change to project folder\n",
        "path = r\"/content/drive/MyDrive/University of Southampton/COMP6247 Reinforcement Learning & Online Learning/RL&OL Assignment\"\n",
        "os.chdir(path)"
      ],
      "metadata": {
        "id": "PtxKa4Zcc_hR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d9fcd55-0e9b-4670-b120-c7f772e38af6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3Jnhi_KN8jW"
      },
      "source": [
        "# Dynamic Maze model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "58LsGza0T4E0"
      },
      "outputs": [],
      "source": [
        "# My dynamic Maze\n",
        "import numpy as np\n",
        "\n",
        "class Objects:\n",
        "  wall=0\n",
        "  free=1\n",
        "  fire=2\n",
        "  goal=3\n",
        "  agent=4\n",
        "\n",
        "# color & index\n",
        "color_wall = (10, 11, 12)    # 0\n",
        "color_free = (224, 224, 224)  # 1\n",
        "color_fire = (255, 0, 0)      # 2\n",
        "color_goal = (51, 255, 51)    # 3\n",
        "colors = [color_wall, color_free, color_fire, color_goal]\n",
        "\n",
        "class DMaze:\n",
        "  def __init__(self, shape, point_start, point_end):\n",
        "    self.height = shape[0]\n",
        "    self.width = shape[1]\n",
        "    self.point_start = point_start\n",
        "    self.point_end = point_end\n",
        "    self.maze = np.zeros(shape, dtype=np.uint8) # stores observed map, assume all walls after init\n",
        "    self._set_cell(point_end, Objects.goal)\n",
        "  \n",
        "  @property\n",
        "  def size(self):\n",
        "    return self.height * self.width\n",
        "  \n",
        "  # update observed maze and return the transformed local values, obs_3x3, \n",
        "  # that use the same format in my dmaze model\n",
        "  def _update_maze(self, center, around):\n",
        "    obs_3x3 = np.zeros(shape=[3, 3], dtype=np.uint8)\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "          # determine the type of the cell\n",
        "          #found = 0\n",
        "          if around[j][i][0]==0:\n",
        "            # find a wall\n",
        "            found = Objects.wall\n",
        "          else:\n",
        "            # if not a wall, it could be fire or free space.\n",
        "            if around[j][i][1]==0:\n",
        "              # no fire, find free space\n",
        "              found = Objects.free\n",
        "            else:\n",
        "              # find a fire\n",
        "              found = Objects.fire\n",
        "          #self.maze[center[1]-1 + j][center[0]-1+i] = found\n",
        "          self._set_cell((center[0]-1+i, center[1]-1 + j), found)\n",
        "          obs_3x3[j][i] = found\n",
        "          # reset goal point\n",
        "          self._set_cell(self.point_end, Objects.goal)\n",
        "    return obs_3x3\n",
        "  \n",
        "  # read maze from file and return transformed values\n",
        "  def observe_around(self, pos):\n",
        "    around = get_local_maze_information(pos[1], pos[0])\n",
        "    return around, self._update_maze(pos, around)\n",
        "  \n",
        "  # get cell type from observed map.\n",
        "  # it will be a wall if the cell never explored.\n",
        "  # assume all walls outside of the maze\n",
        "  def get_cell(self, pos):\n",
        "    if self.is_pos_valid(pos):\n",
        "      return self.maze[pos[1], pos[0]]\n",
        "    # else if out of index, it's a wall\n",
        "    return Objects.wall\n",
        "\n",
        "  def _set_cell(self, pos, value):\n",
        "    if self.is_pos_valid(pos):\n",
        "      self.maze[pos[1]][pos[0]] = value\n",
        "      # else don't update if out of range\n",
        "\n",
        "  def draw(self, rgb_img=None):\n",
        "    if rgb_img== None:\n",
        "      rgb_img = np.empty([self.maze.shape[1], self.maze.shape[0], 3], dtype=np.uint8)\n",
        "    for x in range(self.maze.shape[1]):\n",
        "      for y in range(self.maze.shape[0]):\n",
        "        rgb_img[y][x] = colors[self.maze[y][x]]\n",
        "    return rgb_img\n",
        "\n",
        "  def is_cell_onfire(self, pos):\n",
        "    return self.get_cell(pos) == Objects.fire\n",
        "\n",
        "  def is_pos_valid(self, pos):\n",
        "    x, y = pos\n",
        "    return x>0 and x<self.width-1 and y>0 and y<self.height-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYyBBFqGPFVU"
      },
      "source": [
        "# Q table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OdGz6QOWPI7p"
      },
      "outputs": [],
      "source": [
        "class Qtable:\n",
        "  def __init__(self, num_actions, shape_maze):\n",
        "    self.num_actions=num_actions\n",
        "    self.height = shape_maze[1]\n",
        "    self.width = shape_maze[0]\n",
        "    size_state = self.width * self.height\n",
        "    # create a new table\n",
        "    self.table = np.zeros(shape=(size_state, num_actions), dtype=np.float64)\n",
        "\n",
        "  def reset(self):\n",
        "    self.table = np.zeros(shape=(self.width * self.height, self.num_actions), dtype=np.float64)\n",
        "\n",
        "  def getQ(self, x, y, action_idx):\n",
        "    return self.table[self._state(x, y)][action_idx]\n",
        "  \n",
        "  # get a list of q value for all actions\n",
        "  def getQs(self, x, y):\n",
        "    return self.table[self._state(x, y)]\n",
        "  \n",
        "  def setQ(self, x, y, action_idx, value):\n",
        "    self.table[self._state(x,y)][action_idx] = value\n",
        "\n",
        "  def _state(self, x, y):\n",
        "    return y*self.width + x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEdq3JDTikio"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NNYeLPbNimrz"
      },
      "outputs": [],
      "source": [
        "# Agent\n",
        "\n",
        "class Robot:\n",
        "  actions = [\n",
        "             [0, 0],    # 0 don't move\n",
        "             [0, -1],   # 1 up\n",
        "             [0, 1],   # 2 down\n",
        "             [-1, 0],   # 3 left\n",
        "             [1, 0],   # 4 right\n",
        "  ]\n",
        "  action_space = [0, 1, 2, 3, 4]\n",
        "\n",
        "  def __init__(self, q_table, gamma, epsilon):\n",
        "    self.q_table = q_table\n",
        "    self.gamma = gamma\n",
        "    self._epsilon = epsilon\n",
        "\n",
        "  @property\n",
        "  def position(self):\n",
        "    return [self.x, self.y]\n",
        "\n",
        "  def setPos(self, pos):\n",
        "      self.x=pos[0]\n",
        "      self.y=pos[1]\n",
        "      self.path.append(pos)\n",
        "  \n",
        "  def reset(self):\n",
        "    self.epsilon = self._epsilon\n",
        "    self.path = list()\n",
        "\n",
        "  def updateQ(self, old_posi, new_posi, action, reward, alpha):\n",
        "    x = old_posi[0]\n",
        "    y = old_posi[1]\n",
        "    old_Q = self.q_table.getQ(x, y, action)\n",
        "    if new_posi[0]==199 and new_posi[1]==199:\n",
        "      self.q_table.setQ(x, y, action, 20)\n",
        "    else:\n",
        "      next_maxQ = np.max(self.q_table.getQs(new_posi[0], new_posi[1]))\n",
        "      new_Q = old_Q + alpha * ((reward + self.gamma*next_maxQ) - old_Q)\n",
        "      self.q_table.setQ(x, y, action, new_Q)\n",
        "    return new_Q\n",
        "\n",
        "  # return global position after move\n",
        "  def if_act(self, action):\n",
        "    move = self.actions[action]\n",
        "    new_x = self.x+move[0]\n",
        "    new_y = self.y+move[1]\n",
        "    return (new_x, new_y)\n",
        "\n",
        "  # select an action using greedy method\n",
        "  def choose_action(self, pos, obs_3x3):\n",
        "    valid_actions = Robot.find_valid_actions(obs_3x3)\n",
        "    action = self.greedy_epsilon(pos, obs_3x3, valid_actions)\n",
        "    return action\n",
        "\n",
        "  def find_valid_actions(obs_3x3):\n",
        "    actions = [1, 2, 3, 4]\n",
        "    # up side is not free\n",
        "    if obs_3x3[0][1] == Objects.wall:\n",
        "      actions.remove(1)\n",
        "    # down side\n",
        "    if obs_3x3[2][1] == Objects.wall:\n",
        "      actions.remove(2)\n",
        "    # left\n",
        "    if obs_3x3[1][0] == Objects.wall:\n",
        "      actions.remove(3)\n",
        "    # right\n",
        "    if obs_3x3[1][2] == Objects.wall:\n",
        "      actions.remove(4)\n",
        "    return actions\n",
        "        \n",
        "  # states includes its position and observation\n",
        "  def greedy_epsilon(self, pos, obs, valid_actions):\n",
        "    # option 1: \n",
        "    # choose action by experience\n",
        "    if np.random.random() < self.epsilon:\n",
        "      Qs = list()\n",
        "      for action in valid_actions:\n",
        "        Qs.append(self.q_table.getQ(pos[0], pos[1], action))\n",
        "      \n",
        "      Qs_array = np.array(Qs)\n",
        "\n",
        "      if(len(Qs_array[Qs_array == np.max(Qs_array)]) != 1):\n",
        "        return  valid_actions[np.random.choice(np.arange(0,len(Qs_array[Qs_array == np.max(Qs_array)])))]\n",
        "\n",
        "      return valid_actions[np.argmax(Qs)]\n",
        "    # option 2: \n",
        "    # use a valid random action\n",
        "    else:\n",
        "      action_indices = np.random.choice(valid_actions)\n",
        "      return action_indices\n",
        "\n",
        "  color_agent = (51, 153, 255)\n",
        "  def draw(self, rgb_img):\n",
        "    # draw path\n",
        "    # draw agent\n",
        "    rgb_img[self.y][self.x] = Robot.color_agent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHaRACkArtp2"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GA5Yt8yKrvn8"
      },
      "outputs": [],
      "source": [
        "# MazeEnv\n",
        "class MazeEnv:\n",
        "  def __init__(self, point_start, point_end, size_maze, robot):\n",
        "    self.point_start = point_start\n",
        "    self.point_end = point_end\n",
        "    self.robot = robot\n",
        "    self.size_maze = size_maze\n",
        "    self.dmaze = DMaze(self.size_maze, self.point_start, self.point_end)\n",
        "    # d = dx + dy (dx,dy>0)\n",
        "    self.distance_max = (point_end[0]-point_start[0]) + (point_end[1]-point_start[1])\n",
        "\n",
        "  def step(self, action):\n",
        "    old_posi = self.robot.position\n",
        "    new_posi = self.robot.if_act(action)\n",
        "    self.robot.setPos(new_posi)\n",
        "    reward = self._get_reward(old_posi, new_posi) # act -> reward\n",
        "    around, new_obs = self.dmaze.observe_around(self.robot.position) # observe around 3x3\n",
        "    done = self.is_win()\n",
        "\n",
        "    self.total_rewards += reward\n",
        "    return around, new_obs, reward, done\n",
        "\n",
        "  def reset(self):\n",
        "    self.total_rewards=0\n",
        "    self.robot.reset()\n",
        "    self.robot.setPos(self.point_start)\n",
        "    self.visited = np.zeros(self.size_maze)\n",
        "    around, obs = self.dmaze.observe_around(self.robot.position) # initial state\n",
        "    return around, obs\n",
        "  \n",
        "  # return how many times the bot visited this cell\n",
        "  def robot_visit(self, pos)->int:\n",
        "    x, y = pos\n",
        "    times = self.visited[y][x]\n",
        "    self.visited[y][x] += 1\n",
        "    return times\n",
        "\n",
        "  def is_visited(self, pos):\n",
        "    x, y = pos\n",
        "    return self.visited[y][x] != 0\n",
        "\n",
        "  # state function = robot position\n",
        "  def get_state(self):\n",
        "    return self.robot.position\n",
        "\n",
        "  def _get_reward(self, old_pos, new_pos):\n",
        "    # 1. distance reward\n",
        "    ## calc manhattan distance\n",
        "    distance_old = self.point_end[0]-old_pos[0] + self.point_end[1]-old_pos[1]\n",
        "    distance_now = self.point_end[0]-new_pos[0] + self.point_end[1]-new_pos[1]\n",
        "    ## calc rewards\n",
        "    if distance_old > distance_now:\n",
        "      reward = (-0.0019) * (self.point_end[0]-new_pos[0]) + (-0.0012) * (self.point_end[1]-new_pos[1]) + 3\n",
        "    else:\n",
        "      reward = -1\n",
        "    # 2. repeated step reward\n",
        "    visit_times = self.robot_visit(new_pos)\n",
        "    if visit_times !=0:\n",
        "      reward = -4 - 0.2 * visit_times\n",
        "    return reward\n",
        "\n",
        "  def is_win(self):\n",
        "    return self.robot.x==self.point_end[0] and self.robot.y==self.point_end[1]\n",
        "\n",
        "  # return current maze state in rgb arrays format\n",
        "  def render(self):\n",
        "    img = self.dmaze.draw()\n",
        "    self.robot.draw(img)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVIw7y_MWI3C"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX5XU7ukGBme"
      },
      "source": [
        "## Monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wYqLO1YRJdtt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "import time, os\n",
        "\n",
        "color_path = (30, 120, 200)\n",
        "\n",
        "# Monitor class\n",
        "# intent to data collection, analysis and output\n",
        "class M:\n",
        "  epoch = 0\n",
        "  step = 0\n",
        "  wins = 0\n",
        "  env=None\n",
        "  q_table = None\n",
        "  robot = None\n",
        "  rewards = list()\n",
        "  q_values = list()\n",
        "  steps = list()\n",
        "\n",
        "  # out put for every # steps\n",
        "  def is_output_enabled(epoch, step, done):\n",
        "    return step%5000==0 or done\n",
        "\n",
        "  # excuted at the begining of each epoch\n",
        "  def on_epoch_begin(epoch):\n",
        "    M.epoch = epoch\n",
        "    M.rewards = list()\n",
        "    M.q_values = list()\n",
        "    M.step = 0\n",
        "\n",
        "  # excuted after each step\n",
        "  def on_step_end(epoch, step, pos, around, action, reward, done, q_value):\n",
        "    # log values\n",
        "    M.step=step\n",
        "    M.rewards.append(reward)\n",
        "    M.q_values.append(q_value)\n",
        "    # print logs\n",
        "    if M.is_output_enabled(epoch, step, done):\n",
        "      if step%5000==0:\n",
        "        M.draw_maze(M.env.render)\n",
        "      \n",
        "      print(f'agent pos: {pos}')\n",
        "      print(f'agent q table: {M.q_table.getQs(pos[0], pos[1])}')\n",
        "      print(f'step: {step} action: {action} reward: {reward} total rewards: {M.env.total_rewards}')\n",
        "      print(f'epoch: {epoch} epsilon: {M.robot.epsilon} wins: {M.wins}')\n",
        "      print(f'around: \\n{around[:,:,0]}\\n{around[:,:,1]}')\n",
        "\n",
        "  # run at the end of each epoch\n",
        "  def on_epoch_end(epoch, step):\n",
        "    M.wins += 1\n",
        "    M.steps.append(step)\n",
        "    # export data\n",
        "    dir = f\"./wins_{epoch}/\"\n",
        "    M.export_files(epoch, dir)\n",
        "    M.draw_path(M.env, save_dir=dir)\n",
        "    M.draw_reward_step_plot(dir)\n",
        "    M.draw_q_value_step_plot(dir)\n",
        "    M.draw_step_epoch_plot(epoch, dir)\n",
        "    \n",
        "\n",
        "  def export_files(epoch, dir=\"./export/\"):\n",
        "    print(\"exporting...\")\n",
        "    t = time.localtime()\n",
        "    current_time = time.strftime(\"%H:%M:%S\", t)\n",
        "    print(current_time)\n",
        "    if not os.path.exists(dir):\n",
        "      os.mkdir(dir)\n",
        "    pd.DataFrame(M.env.dmaze.maze).to_csv(dir + \"maze.csv\")\n",
        "    pd.DataFrame(M.robot.path).to_csv(dir + \"robot_path.csv\", mode='a')\n",
        "    pd.DataFrame(M.q_table.table).to_csv(dir + \"q_table.csv\")\n",
        "    f = open(dir+\"info\", \"w\")\n",
        "    f.write(f\"epoch: {epoch} steps: {M.step} total_rewards: {M.env.total_rewards}\")\n",
        "    f.close()\n",
        "\n",
        "  def draw_maze(render):\n",
        "    rgb_img = render()\n",
        "    plt.figure(figsize=(15,15))\n",
        "    plt.imshow(rgb_img, interpolation='nearest')\n",
        "    plt.axis('off')\n",
        "    plt.pause(0.02)\n",
        "  \n",
        "  def draw_path(env, save_dir=None):\n",
        "    img = env.render()\n",
        "    for point in env.robot.path:\n",
        "      x = int(point[1])\n",
        "      y = int(point[0])\n",
        "      # in order to keep the fire point show on the picture\n",
        "      # dont draw path on the point of fire\n",
        "      if img[x][y][0] != color_fire[0] \\\n",
        "          or img[x][y][1] != color_fire[1] \\\n",
        "          or img[x][y][2] != color_fire[2]:\n",
        "        img[x][y] = color_path\n",
        "    plt.figure(figsize=(15,15))\n",
        "    plt.imshow(img, interpolation='nearest')\n",
        "    plt.axis('off')\n",
        "    if save_dir != None:\n",
        "      plt.savefig(save_dir + 'path_plot.png', bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "  def draw_reward_step_plot(dir=None):\n",
        "      # reward vs. step\n",
        "      plt.figure()\n",
        "      plt.plot(M.rewards)\n",
        "      plt.title('Reward vs. Step')\n",
        "      plt.xlabel('Steps')\n",
        "      plt.ylabel('Reward')\n",
        "      if dir != None:\n",
        "        plt.savefig(dir+'reward_step_plot.png')\n",
        "      plt.show()\n",
        "  \n",
        "  def draw_q_value_step_plot(dir=None):\n",
        "      # Q vs. step\n",
        "      plt.figure()\n",
        "      plt.plot(M.q_values)\n",
        "      plt.title('Q-value vs. Step')\n",
        "      plt.xlabel('Steps')\n",
        "      plt.ylabel('Q')\n",
        "      if dir != None:\n",
        "        plt.savefig(dir+'q_value_step_plot.png')\n",
        "      plt.show()\n",
        "\n",
        "  def draw_step_epoch_plot(epoch, dir=None):\n",
        "      # step vs. epoch\n",
        "      plt.figure()\n",
        "      plt.plot(np.arange(0, epoch+1), M.steps)\n",
        "      plt.title('Step vs. Epoch')\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.ylabel('Step')\n",
        "      if dir != None:\n",
        "        plt.savefig(dir+'step_epoch_plot.png')\n",
        "      plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfhFJiL330_Z"
      },
      "source": [
        "## run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Luv32I3qWIiI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa3f0aeb-af75-4efc-b57b-6678b2e9e96e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x1080 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "agent pos: [1, 1]\n",
            "agent q table: [0.      0.      0.      0.      0.47762]\n",
            "step: 0 action: 4 reward: 2.3881 total rewards: 2.3881\n",
            "epoch: 0 epsilon: 0.9 wins: 0\n",
            "around: \n",
            "[[0 0 0]\n",
            " [0 1 1]\n",
            " [0 1 0]]\n",
            "[[0 0 0]\n",
            " [0 0 1]\n",
            " [0 0 0]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c1fcde6391ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# run(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-c1fcde6391ae>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(num_epoch)\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0;31m# 4. action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0maround_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m       \u001b[0;31m## avoid fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdmaze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cell_onfire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-38a766e48345>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetPos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_posi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_posi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_posi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# act -> reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0maround\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdmaze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve_around\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# observe around 3x3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_win\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-99a2adfa3eb1>\u001b[0m in \u001b[0;36mobserve_around\u001b[0;34m(self, pos)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;31m# read maze from file and return transformed values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mobserve_around\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0maround\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_local_maze_information\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maround\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_maze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maround\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/University of Southampton/COMP6247 Reinforcement Learning & Online Learning/RL&OL Assignment/read_maze.py\u001b[0m in \u001b[0;36mget_local_maze_information\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaze_cells\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaze_cells\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mmaze_cells\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "POINT_START = (1, 1) # start point\n",
        "POINT_END = (199, 199) # goal\n",
        "SIZE_MAZE = (201, 201) # maze shape\n",
        "\n",
        "GAMMA = 0.9\n",
        "ALPHA = 0.2\n",
        "EPSILON = 0.9\n",
        "EPSILON_INC_PER_EPOCH = 0.002\n",
        "\n",
        "from read_maze import load_maze, get_local_maze_information\n",
        "\n",
        "def run(num_epoch):\n",
        "  # 1. init q table\n",
        "  q_table = Qtable(num_actions=5, shape_maze=SIZE_MAZE)\n",
        "  # collect data from q_table\n",
        "  M.q_table = q_table\n",
        "  # 2. init agent\n",
        "  robot = Robot(q_table, GAMMA, EPSILON)\n",
        "  M.robot = robot\n",
        "  # 3. init env\n",
        "  env = MazeEnv(POINT_START, POINT_END, SIZE_MAZE, robot)\n",
        "  M.env = env\n",
        "  alpha=ALPHA\n",
        "\n",
        "  for epoch in range(num_epoch):\n",
        "    # around and obs are the same, but in different shape\n",
        "    load_maze()\n",
        "    around, obs = env.reset() \n",
        "    done = False\n",
        "    M.on_epoch_begin(epoch)\n",
        "    step = 0\n",
        "    q_value = 0\n",
        "    # increase epsilon for each epoch\n",
        "    robot.epsilon += epoch * EPSILON_INC_PER_EPOCH\n",
        "    # decrease learning rate each epoch\n",
        "    alpha=alpha*(1-epoch/50)\n",
        "    while not done:\n",
        "      # 1. get current state\n",
        "      old_state = env.get_state() # state == position of robot\n",
        "      # 2. select appropriate aciton\n",
        "      action = robot.choose_action(old_state, obs)\n",
        "      # 3. get next state\n",
        "      new_state = env.get_state()\n",
        "      # 4. action\n",
        "      around_, obs_, reward, done = env.step(action)\n",
        "      ## avoid fire\n",
        "      if not env.dmaze.is_cell_onfire(new_state):\n",
        "        q_value = robot.updateQ(old_state, new_state, action, reward, alpha=alpha)\n",
        "      else:\n",
        "        # find a fire, stay\n",
        "        action = 0\n",
        "      # collect&analysis datas\n",
        "      M.on_step_end(epoch, step, old_state, around, action, reward, done, q_value)\n",
        "      step += 1\n",
        "      # 5. old state = new state\n",
        "      obs = obs_\n",
        "      around = around_\n",
        "      # # debug\n",
        "      # if step > 100:\n",
        "      #   done = True\n",
        "    M.on_epoch_end(epoch, step)\n",
        "# run(1)\n",
        "run(50)\n",
        "print('done')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "RL_Maze_Assignment_Qtable_WenhanWei.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}